<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Paper accepted for publication at ICCV workshops - Team Website</title>
<meta name="description" content="Our paper: “MGiaD: Multigrid in all dimensions. Efficiency and robustness by weight sharing and coarsening in resolution and channel dimensions” by Antonia van Betteray, Matthias Rottmann and Karsten Kahl has been accepted for publication and poster presentation for the workshop on resource efficient deep learning for computer vision at ICCV23.  Current state-of-the-art deep neural networks for image classification consist of 10-100 million learnable parameters, i.e. weights. Despite their high classification accuracy, these networks are highly overparameterized. The complexity of the number of weights can be considered as a function of the number of channels, the spatial extent of the input, and the number of layers of the network. Due to the use of convolutional layers, the scaling of the weight complexity is usually linear with respect to the resolution dimensions, but remains quadratic with respect to the number of channels. Active research in recent years on the use of multigrid-inspired ideas in deep neural networks has shown that, on the one hand, a significant number of weights can be saved by appropriate weight sharing and, on the other hand, that a hierarchical structure in the channel dimension can improve the weight complexity to linear. Exploiting these findings, we introduce an architecture that establishes multigrid structures in all relevant dimensions, contributing to a drastically improved accuracy-parameter trade-off. Our experiments show that this structured reduction in the number of weights reduces overparameterization and additionally improves performance over state-of-the-art ResNet architectures on typical image classification benchmarks.  The paper is available at: openaccess.thecvf.com/content/ICCV2023W/RCV/papers/van_Betteray_MGiaD_Multigrid_in_all_Dimensions._Efficiency_and_Robustness_by_Weight_ICCVW_2023_paper.pdf">


  <meta name="author" content="Marco Schumacher">
  
  <meta property="article:author" content="Marco Schumacher">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Team Website">
<meta property="og:title" content="Paper accepted for publication at ICCV workshops">
<meta property="og:url" content="http://localhost:4000/Team-Website/news/2023-10-1/">


  <meta property="og:description" content="Our paper: “MGiaD: Multigrid in all dimensions. Efficiency and robustness by weight sharing and coarsening in resolution and channel dimensions” by Antonia van Betteray, Matthias Rottmann and Karsten Kahl has been accepted for publication and poster presentation for the workshop on resource efficient deep learning for computer vision at ICCV23.  Current state-of-the-art deep neural networks for image classification consist of 10-100 million learnable parameters, i.e. weights. Despite their high classification accuracy, these networks are highly overparameterized. The complexity of the number of weights can be considered as a function of the number of channels, the spatial extent of the input, and the number of layers of the network. Due to the use of convolutional layers, the scaling of the weight complexity is usually linear with respect to the resolution dimensions, but remains quadratic with respect to the number of channels. Active research in recent years on the use of multigrid-inspired ideas in deep neural networks has shown that, on the one hand, a significant number of weights can be saved by appropriate weight sharing and, on the other hand, that a hierarchical structure in the channel dimension can improve the weight complexity to linear. Exploiting these findings, we introduce an architecture that establishes multigrid structures in all relevant dimensions, contributing to a drastically improved accuracy-parameter trade-off. Our experiments show that this structured reduction in the number of weights reduces overparameterization and additionally improves performance over state-of-the-art ResNet architectures on typical image classification benchmarks.  The paper is available at: openaccess.thecvf.com/content/ICCV2023W/RCV/papers/van_Betteray_MGiaD_Multigrid_in_all_Dimensions._Efficiency_and_Robustness_by_Weight_ICCVW_2023_paper.pdf">







  <meta property="article:published_time" content="2023-10-01T00:00:00+02:00">






<link rel="canonical" href="http://localhost:4000/Team-Website/news/2023-10-1/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/Team-Website/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/Team-Website/feed.xml" type="application/atom+xml" rel="alternate" title="Team Website Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/Team-Website/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/Team-Website/">
          Team Website
          
        </a>
        <ul class="visible-links"></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Paper accepted for publication at ICCV workshops">
    <meta itemprop="description" content="Our paper: “MGiaD: Multigrid in all dimensions. Efficiency and robustness by weight sharing and coarsening in resolution and channel dimensions” by Antonia van Betteray, Matthias Rottmann and Karsten Kahl has been accepted for publication and poster presentation for the workshop on resource efficient deep learning for computer vision at ICCV23.Current state-of-the-art deep neural networks for image classification consist of 10-100 million learnable parameters, i.e. weights. Despite their high classification accuracy, these networks are highly overparameterized. The complexity of the number of weights can be considered as a function of the number of channels, the spatial extent of the input, and the number of layers of the network. Due to the use of convolutional layers, the scaling of the weight complexity is usually linear with respect to the resolution dimensions, but remains quadratic with respect to the number of channels. Active research in recent years on the use of multigrid-inspired ideas in deep neural networks has shown that, on the one hand, a significant number of weights can be saved by appropriate weight sharing and, on the other hand, that a hierarchical structure in the channel dimension can improve the weight complexity to linear. Exploiting these findings, we introduce an architecture that establishes multigrid structures in all relevant dimensions, contributing to a drastically improved accuracy-parameter trade-off. Our experiments show that this structured reduction in the number of weights reduces overparameterization and additionally improves performance over state-of-the-art ResNet architectures on typical image classification benchmarks.The paper is available at: openaccess.thecvf.com/content/ICCV2023W/RCV/papers/van_Betteray_MGiaD_Multigrid_in_all_Dimensions._Efficiency_and_Robustness_by_Weight_ICCVW_2023_paper.pdf">
    <meta itemprop="datePublished" content="2023-10-01T00:00:00+02:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Paper accepted for publication at ICCV workshops
</h1>
          


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p>Our paper: “MGiaD: Multigrid in all dimensions. Efficiency and robustness by weight sharing and coarsening in resolution and channel dimensions” by Antonia van Betteray, Matthias Rottmann and Karsten Kahl has been accepted for publication and poster presentation for the workshop on resource efficient deep learning for computer vision at ICCV23.
<br /><br />
Current state-of-the-art deep neural networks for image classification consist of 10-100 million learnable parameters, i.e. weights. Despite their high classification accuracy, these networks are highly overparameterized. The complexity of the number of weights can be considered as a function of the number of channels, the spatial extent of the input, and the number of layers of the network. Due to the use of convolutional layers, the scaling of the weight complexity is usually linear with respect to the resolution dimensions, but remains quadratic with respect to the number of channels. Active research in recent years on the use of multigrid-inspired ideas in deep neural networks has shown that, on the one hand, a significant number of weights can be saved by appropriate weight sharing and, on the other hand, that a hierarchical structure in the channel dimension can improve the weight complexity to linear. Exploiting these findings, we introduce an architecture that establishes multigrid structures in all relevant dimensions, contributing to a drastically improved accuracy-parameter trade-off. Our experiments show that this structured reduction in the number of weights reduces overparameterization and additionally improves performance over state-of-the-art ResNet architectures on typical image classification benchmarks.
<br /><br />
The paper is available at: openaccess.thecvf.com/content/ICCV2023W/RCV/papers/van_Betteray_MGiaD_Multigrid_in_all_Dimensions._Efficiency_and_Robustness_by_Weight_ICCVW_2023_paper.pdf</p>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-10-01T00:00:00+02:00">October 1, 2023</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/Team-Website/news/2023-9-15/" class="pagination--pager" title="DataNinja association of S. Penquitt and M. Rottmann
">Previous</a>
    
    
      <a href="/Team-Website/news/project3/" class="pagination--pager" title="Making automated driving safer: RELiABEL project launched
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    
      <li><a href="/Team-Website/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Team Website. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/Team-Website/assets/js/main.min.js"></script>










  </body>
</html>
